{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFL Basic Simulation - Results Analysis\n",
    "\n",
    "This notebook provides analysis tools for DFL experiment results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from utils import MetricsTracker, Visualizer\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics from CSV files\n",
    "experiment_name = \"dfl_basic_cifar10\"  # Change this to your experiment name\n",
    "\n",
    "# Load node metrics\n",
    "metrics_file = f\"../results/{experiment_name}_metrics.csv\"\n",
    "if os.path.exists(metrics_file):\n",
    "    metrics_df = pd.read_csv(metrics_file)\n",
    "    print(f\"Loaded node metrics: {metrics_df.shape}\")\n",
    "    print(metrics_df.head())\n",
    "else:\n",
    "    print(f\"Metrics file not found: {metrics_file}\")\n",
    "    metrics_df = None\n",
    "\n",
    "# Load global metrics\n",
    "global_metrics_file = f\"../results/{experiment_name}_global_metrics.csv\"\n",
    "if os.path.exists(global_metrics_file):\n",
    "    global_metrics_df = pd.read_csv(global_metrics_file)\n",
    "    print(f\"\\nLoaded global metrics: {global_metrics_df.shape}\")\n",
    "    print(global_metrics_df.head())\n",
    "else:\n",
    "    print(f\"Global metrics file not found: {global_metrics_file}\")\n",
    "    global_metrics_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if metrics_df is not None:\n",
    "    print(\"=== Experiment Overview ===\")\n",
    "    print(f\"Total rounds: {metrics_df['round'].max()}\")\n",
    "    print(f\"Number of nodes: {metrics_df['node_id'].nunique()}\")\n",
    "    print(f\"Total records: {len(metrics_df)}\")\n",
    "    \n",
    "    print(\"\\n=== Final Round Statistics ===\")\n",
    "    final_round = metrics_df[metrics_df['round'] == metrics_df['round'].max()]\n",
    "    \n",
    "    print(f\"Final Training Accuracy: {final_round['train_accuracy'].mean():.4f} ± {final_round['train_accuracy'].std():.4f}\")\n",
    "    print(f\"Final Test Accuracy: {final_round['test_accuracy'].mean():.4f} ± {final_round['test_accuracy'].std():.4f}\")\n",
    "    print(f\"Final Training Loss: {final_round['train_loss'].mean():.4f} ± {final_round['train_loss'].std():.4f}\")\n",
    "    print(f\"Final Test Loss: {final_round['test_loss'].mean():.4f} ± {final_round['test_loss'].std():.4f}\")\n",
    "    \n",
    "    print(\"\\n=== Data Distribution ===\")\n",
    "    data_sizes = final_round.set_index('node_id')['data_size'].to_dict()\n",
    "    for node_id, size in data_sizes.items():\n",
    "        print(f\"Node {node_id}: {size} samples ({size/sum(data_sizes.values())*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if global_metrics_df is not None:\n",
    "    # Create convergence plot\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    rounds = global_metrics_df['round']\n",
    "    \n",
    "    # Training accuracy\n",
    "    if 'global_train_accuracy' in global_metrics_df.columns:\n",
    "        ax1.plot(rounds, global_metrics_df['global_train_accuracy'], 'b-', linewidth=2, label='Global')\n",
    "        if 'std_train_accuracy' in global_metrics_df.columns:\n",
    "            ax1.fill_between(rounds, \n",
    "                           global_metrics_df['global_train_accuracy'] - global_metrics_df['std_train_accuracy'],\n",
    "                           global_metrics_df['global_train_accuracy'] + global_metrics_df['std_train_accuracy'],\n",
    "                           alpha=0.3)\n",
    "    ax1.set_title('Training Accuracy Convergence')\n",
    "    ax1.set_xlabel('Round')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Test accuracy\n",
    "    if 'global_test_accuracy' in global_metrics_df.columns:\n",
    "        ax2.plot(rounds, global_metrics_df['global_test_accuracy'], 'r-', linewidth=2, label='Global')\n",
    "        if 'std_test_accuracy' in global_metrics_df.columns:\n",
    "            ax2.fill_between(rounds, \n",
    "                           global_metrics_df['global_test_accuracy'] - global_metrics_df['std_test_accuracy'],\n",
    "                           global_metrics_df['global_test_accuracy'] + global_metrics_df['std_test_accuracy'],\n",
    "                           alpha=0.3)\n",
    "    ax2.set_title('Test Accuracy Convergence')\n",
    "    ax2.set_xlabel('Round')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Training loss\n",
    "    if 'global_train_loss' in global_metrics_df.columns:\n",
    "        ax3.plot(rounds, global_metrics_df['global_train_loss'], 'g-', linewidth=2, label='Global')\n",
    "        if 'std_train_loss' in global_metrics_df.columns:\n",
    "            ax3.fill_between(rounds, \n",
    "                           global_metrics_df['global_train_loss'] - global_metrics_df['std_train_loss'],\n",
    "                           global_metrics_df['global_train_loss'] + global_metrics_df['std_train_loss'],\n",
    "                           alpha=0.3)\n",
    "    ax3.set_title('Training Loss Convergence')\n",
    "    ax3.set_xlabel('Round')\n",
    "    ax3.set_ylabel('Loss')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Test loss\n",
    "    if 'global_test_loss' in global_metrics_df.columns:\n",
    "        ax4.plot(rounds, global_metrics_df['global_test_loss'], 'm-', linewidth=2, label='Global')\n",
    "        if 'std_test_loss' in global_metrics_df.columns:\n",
    "            ax4.fill_between(rounds, \n",
    "                           global_metrics_df['global_test_loss'] - global_metrics_df['std_test_loss'],\n",
    "                           global_metrics_df['global_test_loss'] + global_metrics_df['std_test_loss'],\n",
    "                           alpha=0.3)\n",
    "    ax4.set_title('Test Loss Convergence')\n",
    "    ax4.set_xlabel('Round')\n",
    "    ax4.set_ylabel('Loss')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node-wise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if metrics_df is not None:\n",
    "    # Plot individual node performance\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Training accuracy by node\n",
    "    for node_id in metrics_df['node_id'].unique():\n",
    "        node_data = metrics_df[metrics_df['node_id'] == node_id]\n",
    "        ax1.plot(node_data['round'], node_data['train_accuracy'], \n",
    "                label=f'Node {node_id}', alpha=0.7)\n",
    "    ax1.set_title('Training Accuracy by Node')\n",
    "    ax1.set_xlabel('Round')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Test accuracy by node\n",
    "    for node_id in metrics_df['node_id'].unique():\n",
    "        node_data = metrics_df[metrics_df['node_id'] == node_id]\n",
    "        ax2.plot(node_data['round'], node_data['test_accuracy'], \n",
    "                label=f'Node {node_id}', alpha=0.7)\n",
    "    ax2.set_title('Test Accuracy by Node')\n",
    "    ax2.set_xlabel('Round')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training loss by node\n",
    "    for node_id in metrics_df['node_id'].unique():\n",
    "        node_data = metrics_df[metrics_df['node_id'] == node_id]\n",
    "        ax3.plot(node_data['round'], node_data['train_loss'], \n",
    "                label=f'Node {node_id}', alpha=0.7)\n",
    "    ax3.set_title('Training Loss by Node')\n",
    "    ax3.set_xlabel('Round')\n",
    "    ax3.set_ylabel('Loss')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Test loss by node\n",
    "    for node_id in metrics_df['node_id'].unique():\n",
    "        node_data = metrics_df[metrics_df['node_id'] == node_id]\n",
    "        ax4.plot(node_data['round'], node_data['test_loss'], \n",
    "                label=f'Node {node_id}', alpha=0.7)\n",
    "    ax4.set_title('Test Loss by Node')\n",
    "    ax4.set_xlabel('Round')\n",
    "    ax4.set_ylabel('Loss')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if metrics_df is not None:\n",
    "    # Analyze data distribution across nodes\n",
    "    final_round = metrics_df[metrics_df['round'] == metrics_df['round'].max()]\n",
    "    data_sizes = final_round.set_index('node_id')['data_size'].to_dict()\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Bar plot of data sizes\n",
    "    nodes = list(data_sizes.keys())\n",
    "    sizes = list(data_sizes.values())\n",
    "    \n",
    "    bars = ax1.bar(nodes, sizes)\n",
    "    ax1.set_xlabel('Node ID')\n",
    "    ax1.set_ylabel('Dataset Size')\n",
    "    ax1.set_title('Data Distribution Across Nodes')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}', ha='center', va='bottom')\n",
    "    \n",
    "    # Pie chart\n",
    "    ax2.pie(sizes, labels=[f'Node {i}' for i in nodes], autopct='%1.1f%%')\n",
    "    ax2.set_title('Data Distribution (Percentage)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate heterogeneity metrics\n",
    "    sizes_array = np.array(sizes)\n",
    "    mean_size = np.mean(sizes_array)\n",
    "    std_size = np.std(sizes_array)\n",
    "    cv = std_size / mean_size if mean_size > 0 else 0\n",
    "    \n",
    "    print(f\"\\n=== Data Heterogeneity Analysis ===\")\n",
    "    print(f\"Mean dataset size: {mean_size:.1f}\")\n",
    "    print(f\"Standard deviation: {std_size:.1f}\")\n",
    "    print(f\"Coefficient of variation: {cv:.3f}\")\n",
    "    print(f\"Min/Max ratio: {min(sizes)/max(sizes):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if metrics_df is not None:\n",
    "    # Compare final performance across nodes\n",
    "    final_round = metrics_df[metrics_df['round'] == metrics_df['round'].max()]\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Test accuracy distribution\n",
    "    ax1.bar(final_round['node_id'], final_round['test_accuracy'])\n",
    "    ax1.set_xlabel('Node ID')\n",
    "    ax1.set_ylabel('Test Accuracy')\n",
    "    ax1.set_title('Final Test Accuracy by Node')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Test loss distribution\n",
    "    ax2.bar(final_round['node_id'], final_round['test_loss'], color='orange')\n",
    "    ax2.set_xlabel('Node ID')\n",
    "    ax2.set_ylabel('Test Loss')\n",
    "    ax2.set_title('Final Test Loss by Node')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy vs Data Size\n",
    "    ax3.scatter(final_round['data_size'], final_round['test_accuracy'], s=100, alpha=0.7)\n",
    "    ax3.set_xlabel('Dataset Size')\n",
    "    ax3.set_ylabel('Test Accuracy')\n",
    "    ax3.set_title('Test Accuracy vs Dataset Size')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    corr = final_round[['data_size', 'test_accuracy']].corr().iloc[0, 1]\n",
    "    ax3.text(0.05, 0.95, f'Correlation: {corr:.3f}', \n",
    "            transform=ax3.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Loss vs Data Size\n",
    "    ax4.scatter(final_round['data_size'], final_round['test_loss'], s=100, alpha=0.7, color='orange')\n",
    "    ax4.set_xlabel('Dataset Size')\n",
    "    ax4.set_ylabel('Test Loss')\n",
    "    ax4.set_title('Test Loss vs Dataset Size')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    corr = final_round[['data_size', 'test_loss']].corr().iloc[0, 1]\n",
    "    ax4.text(0.05, 0.95, f'Correlation: {corr:.3f}', \n",
    "            transform=ax4.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analysis results\n",
    "if metrics_df is not None and global_metrics_df is not None:\n",
    "    print(\"Saving analysis results...\")\n",
    "    \n",
    "    # Create summary statistics\n",
    "    final_round = metrics_df[metrics_df['round'] == metrics_df['round'].max()]\n",
    "    \n",
    "    summary = {\n",
    "        'experiment_name': experiment_name,\n",
    "        'total_rounds': int(metrics_df['round'].max()),\n",
    "        'num_nodes': int(metrics_df['node_id'].nunique()),\n",
    "        'final_global_test_accuracy': float(global_metrics_df['global_test_accuracy'].iloc[-1]),\n",
    "        'final_global_test_loss': float(global_metrics_df['global_test_loss'].iloc[-1]),\n",
    "        'best_global_test_accuracy': float(global_metrics_df['global_test_accuracy'].max()),\n",
    "        'best_round': int(global_metrics_df.loc[global_metrics_df['global_test_accuracy'].idxmax(), 'round']),\n",
    "        'accuracy_std_final': float(final_round['test_accuracy'].std()),\n",
    "        'loss_std_final': float(final_round['test_loss'].std())\n",
    "    }\n",
    "    \n",
    "    # Save summary\n",
    "    summary_df = pd.DataFrame([summary])\n",
    "    summary_file = f\"../results/{experiment_name}_analysis_summary.csv\"\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    \n",
    "    print(f\"Analysis summary saved to: {summary_file}\")\n",
    "    print(\"\\n=== Final Summary ===\")\n",
    "    for key, value in summary.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}